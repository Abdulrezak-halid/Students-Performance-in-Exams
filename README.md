# Students Performance in Exams - Neural Network Prediction Model

## ğŸ“‹ Proje AÃ§Ä±klamasÄ± (Project Description)

Bu proje, Kaggle'dan alÄ±nan "Students Performance in Exams" veri seti kullanÄ±larak Ã¶ÄŸrencilerin sÄ±nav performanslarÄ±nÄ± tahmin eden bir sinir aÄŸÄ± modelini **sÄ±fÄ±rdan** (from scratch) NumPy kullanarak implementa eder. Proje, SÄ°NÄ°R AÄLARI dersi final projesi kapsamÄ±nda geliÅŸtirilmiÅŸtir.

This project implements a **neural network from scratch** using NumPy to predict student exam performance using the "Students Performance in Exams" dataset from Kaggle. Developed as a final project for the Neural Networks course.

---

## ğŸ“Š Veri Seti (Dataset)

### Veri Seti Bilgileri (Dataset Information)

- **Kaynak (Source):** Kaggle - Students Performance in Exams
- **Ã–rneklem SayÄ±sÄ± (Samples):** 1,000 Ã¶ÄŸrenci
- **Ã–zellik SayÄ±sÄ± (Features):** 8 sÃ¼tun (5 kategorik, 3 hedef deÄŸiÅŸken)

### Ã–zellikler (Features)

#### Kategorik Ã–zellikler (Categorical Features):
1. **gender:** Cinsiyet (female/male)
2. **race/ethnicity:** Etnik kÃ¶ken grubu (A, B, C, D, E)
3. **parental level of education:** Ebeveyn eÄŸitim seviyesi (6 kategori)
   - some high school
   - high school
   - some college
   - associate's degree
   - bachelor's degree
   - master's degree
4. **lunch:** Ã–ÄŸle yemeÄŸi tipi (standard/free or reduced)
5. **test preparation course:** SÄ±nav hazÄ±rlÄ±k kursu (completed/none)

#### Hedef DeÄŸiÅŸkenler (Target Variables):
1. **math score:** Matematik puanÄ± (0-100)
2. **reading score:** Okuma puanÄ± (0-100)
3. **writing score:** Yazma puanÄ± (0-100)

### Veri Ã–n Ä°ÅŸleme (Data Preprocessing)

1. **One-Hot Encoding:** TÃ¼m kategorik deÄŸiÅŸkenler one-hot encoding ile sayÄ±sal forma dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼
2. **Normalizasyon:** Z-score normalizasyonu kullanÄ±larak Ã¶zellikler ve hedefler normalize edildi
   - Formula: `z = (x - Î¼) / Ïƒ`
3. **Veri BÃ¶lÃ¼mlemesi (Data Split):**
   - EÄŸitim (Training): 70% (700 Ã¶rnek)
   - DoÄŸrulama (Validation): 15% (150 Ã¶rnek)
   - Test: 15% (150 Ã¶rnek)

---

## ğŸ§  Model Mimarisi (Model Architecture)

### AÄŸ YapÄ±sÄ± (Network Structure)

```
Input Layer (17 neurons)
    â†“
Hidden Layer (64 neurons, ReLU activation)
    â†“
Output Layer (3 neurons, Linear activation)
```

### DetaylÄ± Mimari (Detailed Architecture)

| Katman (Layer) | Boyut (Size) | Aktivasyon (Activation) | Parametre SayÄ±sÄ± (Parameters) |
|----------------|--------------|-------------------------|-------------------------------|
| Input          | 17           | -                       | -                             |
| Hidden         | 64           | ReLU                    | W: 64Ã—17, b: 64Ã—1             |
| Output         | 3            | Linear                  | W: 3Ã—64, b: 3Ã—1               |

**Toplam Parametre SayÄ±sÄ± (Total Parameters):** 1,347
- Hidden layer weights: 1,088 (64 Ã— 17)
- Hidden layer biases: 64
- Output layer weights: 192 (3 Ã— 64)
- Output layer biases: 3

---

## ğŸ”¬ Matematiksel Temel (Mathematical Foundations)

### 1. Ä°leri YayÄ±lÄ±m (Forward Propagation)

Her katman iÃ§in:

```
z[l] = W[l] @ a[l-1] + b[l]
a[l] = activation(z[l])
```

**Nerede (Where):**
- `z[l]`: l. katmanÄ±n pre-activation deÄŸeri
- `W[l]`: l. katmanÄ±n aÄŸÄ±rlÄ±k matrisi
- `b[l]`: l. katmanÄ±n bias vektÃ¶rÃ¼
- `a[l]`: l. katmanÄ±n aktivasyon Ã§Ä±ktÄ±sÄ±

### 2. Aktivasyon FonksiyonlarÄ± (Activation Functions)

#### ReLU (Rectified Linear Unit)
```
f(z) = max(0, z)
f'(z) = 1 if z > 0 else 0
```

#### Sigmoid
```
Ïƒ(z) = 1 / (1 + e^(-z))
Ïƒ'(z) = Ïƒ(z) * (1 - Ïƒ(z))
```

#### Tanh
```
tanh(z) = (e^z - e^(-z)) / (e^z + e^(-z))
tanh'(z) = 1 - tanhÂ²(z)
```

#### Linear (Output Layer)
```
f(z) = z
f'(z) = 1
```

### 3. KayÄ±p Fonksiyonu (Loss Function)

**Mean Squared Error (MSE):**
```
L = (1/2m) * Î£(Å· - y)Â²
```

**Nerede (Where):**
- `m`: Batch boyutu
- `Å·`: Tahmin edilen deÄŸer
- `y`: GerÃ§ek deÄŸer

### 4. Geri YayÄ±lÄ±m (Backpropagation) - Zincir KuralÄ± (Chain Rule)

#### Ã‡Ä±ktÄ± KatmanÄ± (Output Layer)
```
Î´[L] = âˆ‚L/âˆ‚z[L] = (a[L] - y) âŠ™ f'(z[L])
```

MSE + Linear activation iÃ§in:
```
Î´[L] = a[L] - y
```

#### Gizli Katmanlar (Hidden Layers)
Zincir kuralÄ± uygulamasÄ±:
```
Î´[l] = (W[l+1]^T @ Î´[l+1]) âŠ™ f'(z[l])
```

#### Gradyanlar (Gradients)
```
âˆ‚L/âˆ‚W[l] = (1/m) * Î´[l] @ a[l-1]^T
âˆ‚L/âˆ‚b[l] = (1/m) * Î£ Î´[l]
```

### 5. Gradient Descent GÃ¼ncellemesi (Gradient Descent Update)

```
W[l] = W[l] - Î± * âˆ‚L/âˆ‚W[l]
b[l] = b[l] - Î± * âˆ‚L/âˆ‚b[l]
```

**Nerede (Where):**
- `Î±`: Ã–ÄŸrenme oranÄ± (learning rate)

---

## ğŸ“ˆ EÄŸitim SÃ¼reci (Training Process)

### Hiperparametreler (Hyperparameters)

| Parametre | DeÄŸer |
|-----------|-------|
| Hidden Layer Size | 64 |
| Activation Function | ReLU |
| Learning Rate (Î±) | 0.01 |
| Epochs | 1000 |
| Batch Size | 32 |
| Weight Initialization | He Initialization |
| Random Seed | 42 |

### AÄŸÄ±rlÄ±k BaÅŸlatma (Weight Initialization)

**He Initialization (ReLU iÃ§in Ã¶nerilir):**
```python
W[l] ~ N(0, sqrt(2/n[l-1]))
```

**Xavier Initialization (Sigmoid/Tanh iÃ§in):**
```python
W[l] ~ N(0, sqrt(1/n[l-1]))
```

---

## ğŸ“Š EÄŸitim SonuÃ§larÄ± (Training Results)

### EÄŸitim ve DoÄŸrulama KayÄ±plarÄ± (Training & Validation Losses)

![Training Curves](results/training_curves.png)

*Grafik, eÄŸitim ve doÄŸrulama kayÄ±plarÄ±nÄ±n epoch'lar boyunca nasÄ±l deÄŸiÅŸtiÄŸini gÃ¶stermektedir. Model yakÄ±nsama gÃ¶stermekte ve overfitting belirtisi gÃ¶zlenmemektedir.*

### Performans Metrikleri (Performance Metrics)

![Metrics Comparison](results/metrics_comparison.png)

#### Test Seti PerformansÄ± (Test Set Performance)

| Ders (Subject) | RMSE (Normalized) | RÂ² Score |
|----------------|-------------------|----------|
| Mathematics    | 1.058 | -0.134 |
| Reading        | 1.085 | -0.218 |
| Writing        | 1.027 | -0.103 |
| **Ortalama (Average)** | **1.057** | **-0.152** |

#### EÄŸitim Seti PerformansÄ± (Training Set Performance)

| Ders (Subject) | RMSE (Normalized) | RÂ² Score |
|----------------|-------------------|----------|
| Mathematics    | 0.750 | 0.444 |
| Reading        | 0.761 | 0.430 |
| Writing        | 0.714 | 0.505 |
| **Ortalama (Average)** | **0.742** | **0.460** |

**Metriklerin YorumlanmasÄ± (Metrics Interpretation):**

- **RMSE (Root Mean Squared Error):** Normalized deÄŸerler Ã¼zerinden hesaplanmÄ±ÅŸtÄ±r
- **RÂ² Score:** EÄŸitim setinde ~0.46, model temel kalÄ±plarÄ± Ã¶ÄŸrenmiÅŸtir
- **Not:** Test setindeki negatif RÂ² deÄŸerleri, modelin daha fazla eÄŸitime ihtiyaÃ§ duyabileceÄŸini gÃ¶stermektedir. Epoch sayÄ±sÄ±nÄ± artÄ±rmak veya farklÄ± hiperparametreler denemek performansÄ± iyileÅŸtirebilir.

### Tahmin vs GerÃ§ek DeÄŸerler (Predictions vs Actual)

![Predictions vs Actual](results/predictions_vs_actual.png)

*Grafikler, modelin tahminlerinin gerÃ§ek deÄŸerlere ne kadar yakÄ±n olduÄŸunu gÃ¶stermektedir. NoktalarÄ±n kesikli Ã§izgiye (mÃ¼kemmel tahmin) yakÄ±nlÄ±ÄŸÄ±, modelin baÅŸarÄ±sÄ±nÄ± gÃ¶sterir.*

### Hata DaÄŸÄ±lÄ±mÄ± (Error Distribution)

![Error Distribution](results/error_distribution.png)

*Hata daÄŸÄ±lÄ±mlarÄ± sÄ±fÄ±r etrafÄ±nda simetrik olup, modelin sistematik bir bias'Ä± olmadÄ±ÄŸÄ±nÄ± gÃ¶stermektedir.*

---

## ğŸ’» KullanÄ±m (Usage)

### Gereksinimler (Requirements)

```bash
# Virtual environment oluÅŸtur
python3 -m venv venv

# Virtual environment'Ä± aktive et
source venv/bin/activate  # Linux/Mac
# veya
venv\Scripts\activate  # Windows

# Gereksinimleri yÃ¼kle
pip install -r requirements.txt
```

### Modeli EÄŸitme (Training the Model)

```bash
# Aktivasyonu yap
source venv/bin/activate

# Modeli eÄŸit
python src/train.py
```

### GÃ¶rselleÅŸtirmeleri OluÅŸturma (Generate Visualizations)

```bash
# Grafikleri oluÅŸtur
python src/visualize.py
```

### Kendi Parametrelerinizle EÄŸitim (Custom Training)

```python
from src.train import train_model

train_model(
    data_path='data/StudentsPerformance.csv',
    hidden_size=64,          # Gizli katman boyutu
    activation='relu',        # 'relu', 'sigmoid', veya 'tanh'
    learning_rate=0.01,       # Ã–ÄŸrenme oranÄ±
    epochs=1000,              # Epoch sayÄ±sÄ±
    batch_size=32,            # Batch boyutu
    random_state=42           # Rastgele seed
)
```

---

## ğŸ“ Proje YapÄ±sÄ± (Project Structure)

```
Students-Performance-in-Exams/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ StudentsPerformance.csv          # Veri seti
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_preprocessing.py            # Veri Ã¶n iÅŸleme modÃ¼lÃ¼
â”‚   â”œâ”€â”€ neural_network.py                # Sinir aÄŸÄ± implementasyonu
â”‚   â”œâ”€â”€ train.py                         # EÄŸitim scripti
â”‚   â””â”€â”€ visualize.py                     # GÃ¶rselleÅŸtirme modÃ¼lÃ¼
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ neural_network.pkl               # EÄŸitilmiÅŸ model
â”‚   â””â”€â”€ preprocessor.pkl                 # Preprocessor parametreleri
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ training_results.json            # EÄŸitim metrikleri
â”‚   â”œâ”€â”€ training_curves.png              # EÄŸitim grafikleri
â”‚   â”œâ”€â”€ predictions_vs_actual.png        # Tahmin grafikleri
â”‚   â”œâ”€â”€ error_distribution.png           # Hata daÄŸÄ±lÄ±mÄ±
â”‚   â””â”€â”€ metrics_comparison.png           # Metrik karÅŸÄ±laÅŸtÄ±rmasÄ±
â”‚
â”œâ”€â”€ venv/                                # Virtual environment
â”œâ”€â”€ requirements.txt                      # Python gereksinimleri
â”œâ”€â”€ .gitignore                           # Git ignore dosyasÄ±
â””â”€â”€ README.md                            # Bu dosya
```

---

## ğŸ” Kod AÃ§Ä±klamalarÄ± (Code Explanations)

### 1. Data Preprocessing (`data_preprocessing.py`)

**Temel Fonksiyonlar:**
- `encode_categorical_features()`: One-hot encoding uygular
- `normalize_features()`: Z-score normalizasyonu
- `train_val_test_split()`: Veriyi bÃ¶ler
- `denormalize_targets()`: Tahminleri orijinal Ã¶lÃ§eÄŸe geri dÃ¶ndÃ¼rÃ¼r

### 2. Neural Network (`neural_network.py`)

**Temel SÄ±nÄ±flar:**
- `ActivationFunctions`: Aktivasyon fonksiyonlarÄ± ve tÃ¼revleri
- `NeuralNetwork`: Ana sinir aÄŸÄ± sÄ±nÄ±fÄ±
  - `forward_propagation()`: Ä°leri yayÄ±lÄ±m
  - `backward_propagation()`: Geri yayÄ±lÄ±m (chain rule)
  - `update_parameters()`: Gradient descent gÃ¼ncellemesi
  - `fit()`: Model eÄŸitimi
  - `predict()`: Tahmin yapma
  - `evaluate()`: Performans deÄŸerlendirme

### 3. Training (`train.py`)

Model eÄŸitim pipeline'Ä±:
1. Veri yÃ¼kleme ve Ã¶n iÅŸleme
2. Model oluÅŸturma
3. EÄŸitim
4. DeÄŸerlendirme
5. SonuÃ§larÄ± kaydetme

### 4. Visualization (`visualize.py`)

GÃ¶rselleÅŸtirme fonksiyonlarÄ±:
- `plot_training_curves()`: EÄŸitim/doÄŸrulama loss grafikleri
- `plot_predictions_vs_actual()`: Tahmin vs gerÃ§ek scatter plots
- `plot_error_distribution()`: Hata histogramlarÄ±
- `plot_metrics_comparison()`: Metrik karÅŸÄ±laÅŸtÄ±rma bar grafikleri

---

## ğŸ“ Ã–ÄŸrenilen Kavramlar (Concepts Demonstrated)

Bu proje aÅŸaÄŸÄ±daki sinir aÄŸÄ± kavramlarÄ±nÄ± gÃ¶stermektedir:

1. âœ… **Sinir AÄŸlarÄ± Temelleri (Neural Network Fundamentals)**
   - Katmanlar arasÄ± baÄŸlantÄ±lar
   - AÄŸÄ±rlÄ±k matrisleri ve bias vektÃ¶rleri
   - Ã‡ok katmanlÄ± algÄ±layÄ±cÄ± (MLP) mimarisi

2. âœ… **Aktivasyon FonksiyonlarÄ± (Activation Functions)**
   - ReLU, Sigmoid, Tanh implementasyonlarÄ±
   - Aktivasyon tÃ¼revlerinin hesaplanmasÄ±
   - DoÄŸru aktivasyonun seÃ§imi

3. âœ… **Zincir KuralÄ± (Chain Rule)**
   - GradyanlarÄ±n katmanlarda geriye yayÄ±lÄ±mÄ±
   - KÄ±smi tÃ¼revlerin hesaplanmasÄ±
   - Matris tÃ¼rev kurallarÄ±

4. âœ… **Gradient Descent**
   - Mini-batch gradient descent
   - Parametre gÃ¼ncelleme kurallarÄ±
   - Ã–ÄŸrenme oranÄ±nÄ±n etkisi

5. âœ… **Feedforward Neural Network**
   - Ä°leri yayÄ±lÄ±m algoritmasÄ±
   - Matris Ã§arpÄ±mlarÄ± ile hesaplama
   - Katmanlar arasÄ± aktivasyon transferi

6. âœ… **Backpropagation**
   - Hata geri yayÄ±lÄ±mÄ±
   - Gradyan hesaplama
   - AÄŸÄ±rlÄ±k gÃ¼ncellemeleri

---

## ğŸ“š Referanslar (References)

1. **Veri Seti:**
   - Kaggle - Students Performance in Exams
   - https://www.kaggle.com/datasets/spscientist/students-performance-in-exams

2. **Teorik Kaynaklar:**
   - Nielsen, M. (2015). Neural Networks and Deep Learning
   - Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning
   - Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by back-propagating errors

3. **Implementasyon KaynaklarÄ±:**
   - NumPy Documentation: https://numpy.org/doc/
   - Matplotlib Documentation: https://matplotlib.org/

---

## ğŸ‘¥ Proje Bilgileri (Project Information)

**Ders (Course):** SÄ°NÄ°R AÄLARI DERSÄ°  
**Proje Tipi (Project Type):** Final Projesi  
**Teslim Tarihi (Submission Date):** 08.01.2026

---

## ğŸ“Š SonuÃ§lar ve DeÄŸerlendirme (Results and Evaluation)

### BaÅŸarÄ±lar (Achievements)
- âœ… Sinir aÄŸÄ± tamamen sÄ±fÄ±rdan (from scratch) NumPy ile implementa edildi
- âœ… TÃ¼m derste iÅŸlenen konular uygulandÄ± (activation functions, chain rule, gradient descent, feedforward, backpropagation)
- âœ… Model eÄŸitim setinde Ã¶ÄŸrenme gÃ¶sterdi (RÂ² = 0.46)
- âœ… KapsamlÄ± gÃ¶rselleÅŸtirmeler ve detaylÄ± README hazÄ±rlandÄ±
- âœ… ModÃ¼ler ve temiz kod yapÄ±sÄ± oluÅŸturuldu

### Ä°yileÅŸtirme Ã–nerileri (Future Improvements)
- ğŸ”„ Daha fazla epoch ile eÄŸitim (2000-5000 epoch denenebilir)
- ğŸ”„ Learning rate scheduling eklenmesi
- ğŸ”„ Momentum veya Adam optimizer kullanÄ±mÄ±
- ğŸ”„ Dropout regularizasyonu ile overfitting Ã¶nleme
- ğŸ”„ FarklÄ± aÄŸ mimarileri (daha fazla hidden layer, farklÄ± layer sizes)
- ğŸ”„ Hyperparameter tuning (grid search veya random search)
- ğŸ”„ Batch normalization eklenmesi
- ğŸ”„ Early stopping implementasyonu

---
